No.,Question,Answer
1,Describe the primary goal of the AI Crop Monitoring & Agricultural Intelligence Platform?,Not found in context.
2,What core AI/ML capabilities are integrated into this platform?,"The platform integrates computer vision, time-series forecasting, market analytics, explainable AI, and microfinance scoring."
3,What specific technologies are mentioned for edge computing and cloud-based retraining?,"For edge computing, the technologies mentioned are ONNX, TFLite, ONNX Runtime, tfliteruntime, smartphones, Raspberry Pi, and drones. For cloud-based retraining, the technologies mentioned are Google Colab, Kaggle, and free-tier cloud."
4,"If you were developing a new recommendation engine, in which directory would its production-ready code reside, and where would its initial experimental scripts be?","Its production-ready code would reside in src/, and its initial experimental scripts would be in notebooks/."
5,"Where would you find the configuration for the platform's CI/CD workflows, and what tool is specifically mentioned for defining these workflows?","The configuration for the platform's CI/CD workflows is found in .github/workflows/ci-cd.yml, and the tool specifically mentioned for defining these workflows is GitHub Actions."
6,How does the platform ensure reproducibility and versioning of datasets? Name the specific tool mentioned?,The platform ensures reproducibility and versioning of datasets by versioning datasets with hashes. The specific tool mentioned for dataset versioning is DVC.
7,"Why is Explainable AI considered important for this platform, especially for farmers?",Explainable AI is considered important because it builds trust with farmers.
8,"Explain how the platform supports ""Offline Mode"" for edge inference. What technologies are used to enable this, and where are the results stored locally?","The platform supports offline mode by deploying compressed models like TensorFlow Lite (TFLite) or ONNX on mobile and edge devices such as smartphones, Raspberry Pi, or drones for local inference. The tfliteruntime or ONNX Runtime is used for this inference. The results, including local predictions and recommendations, are stored locally in SQLite on these edge devices."
9,"Describe the ""Continuous Learning"" mechanism, including how farmer feedback is collected and used for model evolution?",Collect farmer feedback into SQLite (offline) and sync to MongoDB Atlas; retrain models monthly using Colab free GPUs. Use active learning strategies like uncertainty sampling.
10,"How does the platform provide ""Digital Finance"" capabilities, and what type of machine learning model is suggested for this? Where is the relevant data stored?","The platform provides Digital Finance capabilities through loan recommendations, which offer financial access. Logistic regression or boosting models are suggested for this. The relevant data is stored in the MongoDB Atlas loanrequests collection."
11,"What specific compliance measure is mentioned, and how is it implemented in terms of data storage and user rights?","GDPR-style logging is mentioned, implemented by encrypting farmer logs in the MongoDB Atlas cropmonitoring collection, and allowing farmers to request deletion. PyMongo is used for data access and audit logs are implemented."
12,"How does the platform address scalability for ""Multi-farm dashboards""?",Build dashboards in Streamlit (src/frontend/streamlitapp/dashboard.py). Fetch data from MongoDB Atlas for multi-farm views. Use Kubernetes for scaling.
13,"Beyond basic offline inference, how does the platform leverage ""Edge AI & On-Farm Inference""? What specific runtimes are used for cross-platform inference?","The platform leverages Edge AI and On-Farm Inference by deploying compressed models (ONNX, TFLite) on smartphones, Raspberry Pi, or drones. ONNX Runtime is used for cross-platform inference."
14,"Describe the concept of ""Adaptive Treatment Optimization"" and the role of MongoDB Atlas in this process. What advanced ML technique is suggested for this?",Adaptive Treatment Optimization involves storing historical actions in MongoDB Atlas to retrain dosage optimization models with new outcomes. The suggested advanced ML technique for this is reinforcement learning.
15,"How does the platform approach ""AI Pest/Disease Forecasting""? What data sources and model types are involved?",The platform approaches AI Pest/Disease Forecasting by using a weather API and an LSTM model to forecast outbreaks. It involves time-series data stored in the MongoDB Atlas yieldforecasts collection.
16,"Explain the ""Satellite + Drone Fusion"" process. What libraries are used for image alignment, and where is the metadata stored?",Process Sentinel-2 via Google Earth Engine or alternatives; fuse with drone images in fusionpipeline.py. Libraries like rasterio are used for alignment. Metadata is stored in MongoDB Atlas imagesmetadata collection.
17,"How is ""Gamification & Community"" implemented, and what MongoDB Atlas collection tracks farmer engagement?",Reward farmers for consistent use (Streamlit/Gradio leaderboards). Points are tracked in the MongoDB Atlas farmerfeedback collection.
18,"What technologies are suggested for ""Voice Interfaces"" to support local languages, and where are voice command logs stored?","Whisper (offline STT) or Vosk are suggested for local languages (Yoruba, Hausa, Igbo). Voice command logs are stored in MongoDB Atlas for analysis."
19,"When adding or updating datasets, what steps are required to maintain ""Licensing & Provenance""? What specific file and tool are used for tracking this?","Maintain manifest.csv with source, license, date, sha256 checksum. Use pandas to generate and validate, store metadata in MongoDB Atlas cropmonitoring collection. Use DVC for dataset versioning. Require farmer consent forms for uploads, storing them digitally in MongoDB Atlas users collection with timestamps. The specific file is manifest.csv and the tools used for tracking are pandas and DVC."
20,"Before training a supervised model, what are the key steps for ""Annotation & Label QA""? What metric is used to ensure quality, and how is it computed?","Use CVAT or Roboflow for annotations, export in COCO or YOLO formats, and store in MongoDB Atlas imagesmetadata collection. Set clear guidelines for resolution (min 1024x1024) and label taxonomy. Ensure inter-annotator agreement of greater than or equal to 0.75 using Cohen’s Kappa, computed with scikit-learn’s cohenkappascore."
21,"How is ""Data Validation"" performed before training and deployment? Name the specific tool used for schema checks and how it integrates with CI/CD?","Data Validation is performed before training and deployment by adding Great Expectations tests for correct file counts and schema checks, defining expectations in greatexpectations/expectations/, validating MongoDB Atlas data, and running pytest in CI/CD pipelines. The specific tool used for schema checks is Great Expectations. It integrates with CI/CD by running pytest in CI/CD pipelines, which integrates with GitHub Actions for automated validation."
22,"What is a ""Model Card,"" and when is it generated? How is it stored and tracked within the platform's ecosystem?","A Model Card is a document (model-card.md) that contains the intended use, limitations, and metrics (accuracy, F1-score) of a model. It is generated at each model release. It is saved in the MongoDB Atlas cropmonitoring collection and tracked in the MLflow registry, with metadata synced to MongoDB Atlas for multi-user access."
23,"Explain the process for ""Drift Detection & Retraining."" What tool is used to detect distribution drifts, and under what condition would retraining be triggered? How is this automated?",Evidently AI is used to detect distribution drifts. Retraining is triggered if macro-F1 drops below baseline. This process is automated with cron jobs or Airflow.
24,What specific MongoDB Atlas feature is mentioned for enhancing data encryption?,MongoDB Atlas client-side field-level encryption.
25,What are the defined Service Level Objectives (SLOs) for latency and uptime?,"Latency p95 is less than 200ms, and uptime is greater than or equal to 99.5%."
26,"How is ""Observability"" achieved, and where are metrics stored?","Observability is achieved by monitoring with Prometheus, Grafana, and OpenTelemetry. Metrics are stored in the MongoDB Atlas systemlogs collection."
27,"Describe the ""CI/CD & Testing Gates"" process. What specific checks block deployment, and how are staged deployments managed?","Before merging to production, GitHub Actions run tests (pytest, lint with black/flake8). Deployment is blocked if model accuracy drops below baseline, using conditional steps in workflows. Staged deployments are managed through canary rollouts with rollback triggers, implemented using Kubernetes deployments."
28,"What practices ensure ""Reproducibility"" for training runs and releases?","Pin seeds and dependencies using requirements.txt or pyproject.toml, and use random.seed(42) in code. Version datasets with hashes using DVC or Git LFS, storing metadata in MongoDB Atlas. Backup MongoDB Atlas weekly with mongodump for exports, automated via cron."
29,List at least three key MLOps tools or concepts a developer should be familiar with for this project?,"MLflow for experiment tracking, Docker/Kubernetes for deployment, and Prometheus/Grafana for monitoring."
30,"What specific techniques are mentioned for reducing model size for ""Edge AI"" deployment?",Pruning and Quantization are mentioned for reducing model size.
31,"Where would you find the code for the main Streamlit dashboard, and where would reusable UI components for a Next.js application be located?",The main Streamlit dashboard code is found in src/frontend/streamlitapp/dashboard.py. Reusable UI components for a Next.js application are located in src/frontend/nextjsapp/components/.
32,"How are Streamlit, Next.js, and React Native applications typically deployed and tested?",Deploy Streamlit on Heroku or AWS; Next.js on Vercel; React Native via Expo or direct builds. Testing involves Jest for unit tests and Cypress for E2E tests.
33,Analyze the provided Streamlit code snippet. What is its purpose?,To handle crop image uploads and display recommendations.
34,"What type of file is it designed to upload, and what is the expected response from the backend?",It is designed to upload jpg or png image files. The backend is expected to return a JSON object containing disease and confidence.
35,"What is the primary API framework used for the backend, and why is it chosen?","FastAPI is the asynchronous API framework used for the backend, chosen for high performance."
36,"What tool is used for asynchronous jobs, and what is the main database technology?","Celery is used for asynchronous jobs, and MongoDB Atlas is the primary database."
37,List at least five specific MongoDB Atlas collections mentioned in the backend architecture?,"Imagesmetadata, farmerfeedback, marketprices, loanrequests, notifications, yieldforecasts, systemlogs"
38,Describe the functionality of the /api/upload and /api/recommendations endpoints?,"The /api/upload endpoint handles image uploads, triggers inference, and stores metadata in MongoDB Atlas imagesmetadata. The /api/recommendations endpoint fetches personalized suggestions from MongoDB Atlas based on user ID."
39,"What are ""Background Services"" in the backend, and what are two examples of tasks they perform?",Background Services are dedicated scripts for compute-intensive tasks. Two examples of tasks they perform are scraping market data and running inference for models.
40,Analyze the provided FastAPI code snippet. What is its core function?,"The core function of the provided FastAPI code snippet is to handle image uploads, perform inference (prediction) on the uploaded image using a TensorFlow model, store the image metadata and prediction result in MongoDB Atlas, and return the prediction result."
41,How does it interact with MongoDB Atlas?,"The system interacts with MongoDB Atlas as a centralized cloud database for multi-user access, dashboards, analytics, and persistent storage. It stores various data including images metadata, farmer feedback, market prices, loan requests, yield forecasts, notifications, and system logs. Edge devices sync offline data from SQLite to MongoDB Atlas when online, and backend services and dashboards read from and write to MongoDB Atlas for operations, monitoring, and compliance."
42,"What machine learning operation is performed, and what is stored in the database after a successful operation?","AI Pest/Disease Forecasting using an LSTM model is performed, and time-series data is stored in the MongoDB Atlas yieldforecasts collection."
43,What is the purpose of UploadFile = File(...) and await file.read()?,UploadFile = File(...) is used to define a parameter for an uploaded file in the /api/inference endpoint. Await file.read() is used to read the content of the uploaded file.
44,How would you run the FastAPI backend locally?,Run locally with uvicorn backend.api.main:app –reload.
45,What tools are recommended for scaling the backend and for load testing?,"For scaling the backend, Gunicorn and Docker containers are recommended. For load testing, Locust is recommended."
46,"Explain the role of SQLite on edge devices. What kind of data is stored there, and why is it preferred over MongoDB Atlas for local storage?","SQLite serves as temporary, local storage on mobile and edge devices like smartphones, Raspberry Pi, or drones. It supports offline-first operations, preventing farm use from being blocked when there is no internet. Data stored includes uploaded images before syncing to MongoDB Atlas, local predictions and recommendations, and sensor readings or drone metadata."
47,"Describe the ""Combined Workflow"" for data synchronization between edge devices and MongoDB Atlas. What are the key benefits of this approach?","The Combined Workflow for data synchronization involves edge devices operating in offline mode by saving images, metadata, local TFLite/ONNX inference results, and recommendations/feedback to SQLite. When online, the sync process pushes these SQLite rows to MongoDB Atlas, confirms success, and then clears the synced SQLite rows. Dashboards and the backend then read from MongoDB Atlas. The key benefits of this approach are that it ensures offline-first usability for farmers, maintains centralized data in MongoDB Atlas for analytics, dashboards, and retraining, and is low-cost and free-tier compatible with MongoDB Atlas M0."
48,"What tools are used for monitoring API latency, model performance, and system health? Where are these metrics and alerts stored?","Prometheus and Grafana are used for monitoring API latency, model performance, and system health. These metrics are stored in the MongoDB Atlas systemlogs collection, and alerts are stored in the MongoDB Atlas notifications collection."
49,How is distributed tracing achieved across services?,OpenTelemetry is used for distributed tracing across services.
50,How are ML experiments and model metadata tracked?,MLflow is used to track experiments locally and sync metadata to MongoDB Atlas cropmonitoring for multi-user setups. Model Cards are stored in MongoDB Atlas with metrics and limitations.
51,"What is the role of GitHub Actions in the CI/CD pipeline, particularly regarding model accuracy?","GitHub Actions run tests, linting, and model accuracy checks before deployment. Deployment is blocked if model accuracy falls below a baseline."
52,What are the core features expected to be delivered in the MVP (0-3 months) phase?,"Dataset setup (PlantVillage), train CNN (ResNet50), build Streamlit app for farmers, and feedback collection in SQLite (offline) with sync to MongoDB Atlas farmerfeedback."
53,Which database technologies are central to the MVP's data strategy for both online and offline operations?,"MongoDB Atlas and SQLite are central to the MVP's data strategy for both online and offline operations. MongoDB Atlas serves as the centralized cloud database for multi-user access, dashboards, analytics, and persistent storage, while SQLite is used for temporary, local storage on mobile/edge devices to support offline-first operations."
54,"Explain how the platform is designed to be ""Free-Tier Friendly."" Name at least three specific free-tier services or strategies mentioned?","The platform is designed to be free-tier friendly by not requiring payment or ATM verification for the MVP, leveraging free tiers of services like MongoDB Atlas M0 for storage, Google Colab or Kaggle for free GPUs for model training, and using edge devices with SQLite for offline-first local storage that syncs to MongoDB Atlas when online. It also utilizes AWS Free Tier (EC2 t2.micro, S3) and open APIs."
55,"What are the limitations of the MongoDB Atlas Free-Tier (M0), and why is it considered sufficient for the MVP?","512MB storage, sufficient for MVP data and multi-farm prototype. Scales automatically for small datasets."
56,What formats do these sources typically provide?,These sources provide .SAFE or GeoTIFF formats.
57,"If live Sentinel-2 data is unavailable for the MVP, how can local simulation be performed for raster images and feature extraction?","If live Sentinel-2 access is unavailable for the MVP, local simulation can be performed for raster images by using sample .tif or .png bands from AWS/Copernicus datasets, storing them in data/satellite/ for local processing. For feature extraction, precompute indices like NDVI, EVI using rasterio or GDAL, and save them as satellitefeatures.csv for ML model input."
58,What is the recommended MVP strategy for satellite data?,Start offline-first using SQLite plus sample Sentinel-2 rasters or CSVs. Sync all offline outputs to MongoDB Atlas when network is available. Replace simulation with real satellite feeds when Google Earth Engine or other APIs become accessible.
59,List at least five external APIs or integration methods used by the platform?,"Open WeatherMap API, Twilio API, Gmail API, BeautifulSoup/Selenium, FAO APIs."
60,"What types of ""Edge Devices"" are targeted for on-farm inference?","Raspberry Pi 4, drones like DJI with cameras."
61,"Beyond unit and integration tests, what other types of testing are emphasized for Quality Assurance, and what specific security checks are mentioned?","Beyond unit and integration tests, performance testing to measure inference time and scalability is emphasized for Quality Assurance. Specific security checks mentioned include OWASP scans and dependency checks with pip-audit."
62,"What are two ""Additional Considerations"" highlighted for the project's long-term success or impact?","Cost Management: Use free tiers of MongoDB Atlas, AWS, and open APIs. Localization: Support Yoruba, Hausa, Igbo in UI (React Intl). Sustainability: Optimize models for low energy on edge devices."
