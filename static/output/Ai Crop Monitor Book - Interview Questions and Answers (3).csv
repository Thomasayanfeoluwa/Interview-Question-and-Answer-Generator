No.,Question,Answer
1,"Platform Overview: Describe the core purpose and key integrated technologies of the AI Crop Monitoring & Agricultural Intelligence Platform. Who are its primary target users, and how does it cater to their specific needs?","The AI Crop Monitoring & Agricultural Intelligence Platform is a professional-grade ecosystem for precision agriculture. It integrates computer vision, time-series forecasting, market analytics, explainable AI, and microfinance scoring into a single, modular, and scalable system, combining edge computing with cloud-based retraining. It is designed to serve Farmers with a simple mobile/web interface, Cooperatives/NGOs with dashboards for group management, and Enterprises with large-scale deployments for monitoring, compliance, and supply chain integration."
2,"Hybrid Architecture: Explain the hybrid architecture of the platform, combining edge computing with cloud-based retraining. What are the main benefits of this approach in the context of precision agriculture?","The system combines edge computing (offline models on phones, Raspberry Pi, or drones) with cloud-based retraining (Google Colab, Kaggle, or free-tier cloud). This makes it both low-cost and scalable."
3,Which specific tools or frameworks are recommended for generating documentation from Markdown files within the docs/ directory?,Utilize Sphinx or MkDocs for generating documentation from Markdown files.
4,"What is the purpose of integrating Explainable AI (XAI) features like SHAP, LIME, and Grad-CAM into the platform?",Builds trust with farmers.
5,Describe how a developer would typically use Grad-CAM and SHAP in this project?,"A developer would use Grad-CAM in notebooks to show heatmaps and SHAP on structured models, such as for credit risk. Both would be integrated into dashboards for visual explanations."
6,"How does the platform implement continuous learning, and what role does farmer feedback play in this process?","The platform implements continuous learning through feedback loops and active learning strategies. Farmer feedback is collected into SQLite for offline storage, then synced to MongoDB Atlas, and used to retrain models monthly using Colab free GPUs."
7,"Explain the mechanism for enabling offline mode for edge inference, including the model formats and local storage used?","For enabling offline mode for edge inference, compressed models in TFLite or ONNX formats are deployed on edge devices such as smartphones, Raspberry Pi, or drones for local inference. The tfliteruntime or ONNX Runtime is used for this inference. Local storage on these edge devices for offline data, predictions, and recommendations is handled by SQLite. JSON is an alternative lightweight format for simple metadata if SQLite is too heavy. When network is available, the data stored in SQLite or JSON is synced to MongoDB Atlas."
8,Describe the process for collecting market price data and how it's used to maximize farmer profit. What technologies are involved?,Collect price data via scraping using BeautifulSoup/Selenium. This data is then used to apply regression models like LinearRegression from scikit-learn to maximize farmer profit. The scraped data is stored in the MongoDB Atlas marketprices collection and scheduled with Celery.
9,"How does the platform provide financial access through loan recommendations, and where is this data stored?",The platform provides financial access by training logistic regression or boosting models for loan recommendations and deploying them via backend/api/recommendations.py. This data is stored in the MongoDB Atlas loanrequests collection.
10,How does the platform achieve scalability for multi-farm dashboards?,Build dashboards in Streamlit. Fetch data from MongoDB Atlas for multi-farm views. Use Kubernetes for scaling.
11,What specific measures are taken to ensure GDPR-style logging and data privacy?,Encrypt farmer logs in MongoDB Atlas cropmonitoring collection; allow farmers to request deletion. Use PyMongo for data access and implement audit logs.
12,"How does the platform support ""Adaptive Treatment Optimization,"" and what advanced ML technique is mentioned for this?",The platform supports Adaptive Treatment Optimization by storing historical actions in MongoDB Atlas and then retraining the dosage optimization model with new outcomes. The advanced ML technique mentioned for this is reinforcement learning techniques.
13,"Describe the ""Satellite + Drone Fusion"" process, including the data sources and libraries involved?",The Satellite + Drone Fusion process involves processing Sentinel-2 data via Google Earth Engine or alternatives and fusing it with drone images in a fusion script using libraries like rasterio and GDAL for alignment. Metadata is stored in the MongoDB Atlas imagesmetadata collection.
14,"How is the ""Farm Health Score"" computed and displayed within the platform?","It is computed as a weighted index of disease percentage, soil, and rainfall in backend/services/healthscore.py, and stored in the MongoDB Atlas farms collection for dashboard display."
15,How does the platform incorporate gamification and community features? Where is the relevant data tracked and displayed?,Reward farmers for consistent use (Streamlit/Gradio leaderboards). Points are tracked in MongoDB Atlas farmerfeedback collection and top users are displayed.
16,"What technologies are suggested for implementing voice interfaces, especially for local languages?",Whisper (offline STT) or Vosk are suggested for implementing voice interfaces for local languages. Vosk.js is also mentioned for voice in web.
17,"How is data validation performed before training and deployment, and what tool is specifically mentioned for schema checks?","Data validation is performed before training and deployment by adding Great Expectations tests for correct file counts and schema checks, and by running pytest in CI/CD pipelines integrated with GitHub Actions for automated validation. The tool specifically mentioned for schema checks is Great Expectations."
18,"What is a ""model card,"" and how is it used and tracked within the platform's model governance strategy?","A model card is a document (model-card.md) that stores the intended use, limitations, and metrics such as accuracy and F1-score for a model. It is saved in the MongoDB Atlas cropmonitoring collection and tracked in the MLflow registry, with metadata synced to MongoDB Atlas for multi-user access."
19,How does the platform detect distribution drifts in models after deployment?,The platform uses Evidently AI to detect distribution drifts.
20,"What action is taken if the macro-F1 score drops below a baseline, and how is this automated?",Retrain if macro-F1 drops below baseline. Automate with cron jobs or Airflow.
21,What are the stated Service Level Objectives (SLOs) for latency and uptime?,"Set latency p95 <200ms, uptime ≥99.5%."
22,"Describe the CI/CD testing gates. What conditions would block a deployment, and how are canary rollouts implemented?","GitHub Actions run tests (pytest, lint with black/flake8). Deployment is blocked if model accuracy drops below baseline. Canary rollouts with rollback triggers are implemented using Kubernetes deployments."
23,"Reproducibility & Backups: How is reproducibility ensured for training runs and releases? What is the backup strategy for MongoDB Atlas, including RPO and RTO?","Reproducibility is ensured by pinning seeds and dependencies (requirements.txt, pyproject.toml) using random.seed(42) in code, and versioning datasets with hashes using DVC or Git LFS, storing metadata in MongoDB Atlas. MongoDB Atlas is backed up weekly with an RPO of 1 day and an RTO of 4 hours, using mongodump for exports, automated via cron."
24,"Mobile Specific Features: What specific features are highlighted for the mobile application, particularly concerning camera integration and offline capabilities?","Camera integration for direct captures, offline mode with local storage (SQLite via Expo SQLite), sync when online to MongoDB Atlas."
25,Explain the purpose of this Streamlit code snippet?,"The Streamlit code snippet is for crop image upload, triggering inference, and displaying the recommendation received from the backend."
26,"What external service or component does this snippet interact with, and how?",The snippet interacts with MongoDB Atlas by connecting to it using MongoClient and inserting data into the imagesmetadata collection within the cropmonitoring database.
27,"Frontend Deployment & Testing: How are Streamlit, Next.js, and React Native applications deployed and tested? What is the accessibility compliance goal?",Streamlit is deployed on Heroku or AWS; Next.js on Vercel; React Native via Expo or direct builds. Testing involves Jest for unit tests and Cypress for E2E tests. The accessibility compliance goal is to ensure WCAG compliance with ARIA labels.
28,"Backend Technologies & Purpose: What are the primary technologies and frameworks used in the backend, and what is the role of each (e.g., API, asynchronous jobs, database)?","FastAPI for asynchronous API framework and high performance, Celery for task queue for asynchronous jobs like model training or scraping, MongoDB Atlas via pymongo for centralized storage, Workers for dedicated scripts for compute-intensive tasks, and FastAPI Security with JWT for authentication and role-based access."
29,Describe the functionality of the /api/upload and /api/recommendations endpoints?,"/api/upload handles image upload, triggers inference, and stores metadata in MongoDB Atlas imagesmetadata. /api/recommendations fetches personalized suggestions from MongoDB Atlas based on user ID."
30,"What are ""Background Services"" in the backend, and what tasks do they handle?",Background Services are a scraper worker for market data and an inference worker for models.
31,Explain the flow of data and operations when an image is uploaded to the /api/inference endpoint?,"When an image is uploaded to the /api/inference endpoint, the image content is read and decoded. A model then performs a prediction on the image. The prediction result, including the detected disease and its confidence, is extracted. This result, along with the image filename, is stored in the MongoDB Atlas imagesmetadata collection. Finally, the prediction result is returned."
32,What are the key dependencies imported and how are they used?,"Key dependencies include FastAPI for asynchronous API development, Streamlit for rapid dashboard prototyping, and TensorFlow or PyTorch for model training. Scikit-learn is used for various machine learning tasks like market pricing and clustering, while Prophet is for yield forecasting. OpenCV-Python handles image processing. MLflow tracks experiments and model metadata, and DVC manages dataset versioning. Data validation is done with Great Expectations, and drift detection with Evidently. PyMongo provides data access to MongoDB Atlas, and Celery handles asynchronous jobs. Twilio is for SMS and WhatsApp notifications, and BeautifulSoup and Selenium are for web scraping market prices. Rasterio is used for satellite and drone image fusion. For edge devices, ONNX and TensorFlow Lite Runtime compress and deploy models for local inference. Vosk enables voice interfaces for local languages, and requests is used for external API calls. Pandas helps generate and validate CSV manifests, and Sentinelhub, Sentinelsat, and Boto3 are for accessing Sentinel-2 satellite data."
33,How does this endpoint interact with MongoDB Atlas?,The /api/upload endpoint stores metadata in MongoDB Atlas imagesmetadata.
34,"Backend Deployment & Testing: How would you run the FastAPI backend locally, and what tools are recommended for scaling and load testing?","Run locally with uvicorn backend.api.main:app –reload. Scale with Gunicorn, deploy in Docker containers. For testing, Pytest for APIs and Locust for load testing are recommended."
35,"Caching Strategy: What is the primary local storage solution for offline data on edge devices, and what is an alternative lightweight format mentioned?","SQLite is the primary local storage on edge devices for offline data, and JSON is an alternative lightweight format for simple metadata."
36,"Monitoring Tools: Name the tools used for tracking API latency, model performance, and system health. Where are these metrics stored?","Prometheus and Grafana are used for tracking API latency, model performance, and system health. These metrics are stored in the MongoDB Atlas systemlogs collection."
37,"CI/CD Workflow: Explain the role of GitHub Actions in the CI/CD pipeline. What specific checks are performed before deployment, and what happens if model accuracy falls below a baseline?","GitHub Actions run tests (pytest, lint with black/flake8) before merging to production. Deployment is blocked if model accuracy falls below a baseline."
38,MVP Deliverables: List the key deliverables for the Minimum Viable Product (MVP) phase (0-3 months). What core functionalities are established during this period?,"Dataset setup (PlantVillage), training a CNN (ResNet50), building a Streamlit app for farmers with upload and inference capabilities, and feedback collection in SQLite (offline) with syncing to MongoDB Atlas farmerfeedback."
39,Phase 2 & 3 Features: What new AI/ML capabilities are introduced in Phase 2 (3-6 months) and Phase 3 (6-12 months) of the implementation roadmap?,"In Phase 2 (3-6 months), new AI/ML capabilities include yield forecasting using Prophet/LSTM, a market pricing model, and microfinance scoring with logistic regression. In Phase 3 (6-12 months), capabilities added are the integration of satellite and drone pipelines with a fusion script, and voice interfaces using Vosk integration."
40,Database Roles: Clearly differentiate the roles of MongoDB Atlas and SQLite in the overall database and offline-edge strategy. Why is each chosen for its specific purpose?,"MongoDB Atlas serves as the centralized cloud database for multi-user access, dashboards, analytics, and persistent storage, chosen because all multi-user, cross-device, or long-term analytics are stored there. SQLite is used for temporary, local storage on mobile/edge devices, chosen to support offline-first operations and avoid blocking farm use when there is no internet."
41,"Combined Workflow Benefits: What are the primary benefits of the combined workflow involving edge devices, SQLite, and MongoDB Atlas?","Ensures offline-first usability for farmers. Maintains centralized data in MongoDB Atlas for analytics, dashboards, and retraining. Low-cost, free-tier compatible with MongoDB Atlas M0."
42,"Sentinel-2 Alternatives: Since Google Earth Engine registration is not possible, list at least three alternative sources for free Sentinel-2 data. How would a developer access data from one of these alternatives?","Since Google Earth Engine registration is not possible, alternative sources for free Sentinel-2 data include AWS Open Data, Copernicus Open Access Hub, and USGS EarthExplorer. A developer would access data from AWS Open Data via boto3 using s3 = boto3.client('s3'); s3.downloadfile('sentinel-s2-l1c', path, local)."
43,"Hardware & Infrastructure: What types of edge devices are recommended, and what cloud services are utilized for the platform's infrastructure?","Edge devices recommended are Raspberry Pi 4 (for on-farm inference) and drones like DJI with cameras. Cloud services utilized include AWS Free Tier (EC2 t2.micro, S3), MongoDB Atlas M0, and alternatives like DigitalOcean."
44,"Testing & QA: Beyond unit tests, what other types of testing are emphasized for quality assurance, and what specific security checks are mentioned?","Beyond unit tests, other types of testing emphasized for quality assurance include integration tests for API chains with MongoDB Atlas, performance tests to measure inference time and scalability, linting with black/flake8, and model accuracy checks. Specific security checks mentioned are OWASP scans and dependency checks with pip-audit."
45,"Additional Considerations: What are the three ""Additional Considerations"" highlighted for the project, and why are they important?","Cost Management: Use free tiers of MongoDB Atlas, AWS, and open APIs. Localization: Support Yoruba, Hausa, Igbo in UI (React Intl). Sustainability: Optimize models for low energy on edge devices."
